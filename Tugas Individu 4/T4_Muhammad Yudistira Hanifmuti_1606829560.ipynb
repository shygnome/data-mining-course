{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sgcqWVxXUzw"
   },
   "source": [
    "<h2><center>Tugas Individu 4 Data Mining 2019/2020</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OAAl2YX3XhXY"
   },
   "source": [
    "\n",
    "<center>Tanggal Launching: 5 November 2019<center>\n",
    "<center><font color=\"red\"><b>Deadline: 14 November 2019, pukul 22.00</b></font><center>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWjGg2x4XUz4"
   },
   "source": [
    "<h4>Nama          : Muhammad Yudistira Hanifmuti </h4>\n",
    "<h4>NPM           : 1606829560 </h4>\n",
    "<h4>Kolaborator   : </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aKax0_7QXUz6"
   },
   "source": [
    "Petunjuk umum:<br>\n",
    "\n",
    "1. Dataset yang digunakan pada tugas ini merupakan dataset yang sama dengan tugas 3, yaitu dataset diabetes yang dapat diunduh di https://bit.ly/2M5vioB\n",
    "\n",
    "2. Lakukan pengolahan data dan perhitungan menggunakan bahasa pemrograman Python. Gunakan template Jupyter notebook yang telah disediakan untuk menjawab soal.\n",
    "\n",
    "3. Penggunaan Library dibatasi hanya untuk penggunaan numpy array dan pandas untuk membaca data. Khusus untuk implementasi Multilayer Perceptron, Anda dapat menggunakan library (Sklearn, Keras atau library lain).\n",
    "\n",
    "4. Format penulisan nama file di Jupyter notebook: T4_Nama_NIM.ipynb\n",
    "\n",
    "5. Kumpulkan pada slot yang disediakan di scele. Deadline: 14 November 2019, 22.00 WIB.\n",
    "\n",
    "6. Jika dalam menyelesaikan tugas ini anda berkolaborasi dengan orang lain, silahkan dituliskan dengan siapa anda berkolaborasi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yxdl4DWbXUz-"
   },
   "source": [
    "<h5>*) Tulis jawaban ditempat yang sudah disediakan</h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2XF0rOfXU0D"
   },
   "source": [
    "<b><i>Neural Network</i></b>\n",
    "\n",
    "1. Bagilah dataset diabetes menjadi training dan testing dengan proporsi 80:20. Buatlah model Perceptron untuk mengklasifikasikan dataset tersebut. Gunakan <i> online learning </i> untuk meng<i>update weight </i> dan bias. Jelaskan fungsi aktivasi yang Anda gunakan dan bagaimana Anda memilih parameter yang optimal (misal <i>learning rate</i>).\n",
    "2. Dari hasil prediksi diatas, hitunglah akurasi, precision dan recall pada tiap kelas. Jika Anda melakukan variasi parameter, tampilkan hasil metrik evaluasi tersebut pada setiap variasi parameter yang Anda gunakan (bisa berupa tabel atau grafik)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "OEPOX0d9XU0N"
   },
   "source": [
    "Jawaban:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "text",
    "id": "6mQHstDpXU0T",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "Fungsi aktivasi yang digunakan pada model Perceptron ini adalah step function sederhana.\n",
      "Fungsi aktivasi menentukan hasil klasifikasi berdasarkan `sign` atau `tanda` dari hasil jumlahan\n",
      "perkalian weights dengan data saja.\n",
      "\n",
      "Untuk pemilihan parameter learning rate, digunakan linear search atau mencoba satu per satu\n",
      "nilai dari learning rate yang memungkinkan. Untuk setiap hasil model dengan learning rate\n",
      "masing-masing akan dibandingkan nilai akurasinya.\n",
      "\u001b[0m\n",
      "\n",
      "Fitting/Learning process is running ...\n",
      "Epoch     1 avg_err =    0.435\n",
      "Epoch     2 avg_err =    0.433\n",
      "Epoch     3 avg_err =    0.428\n",
      "Epoch     4 avg_err =    0.427\n",
      "Epoch     5 avg_err =    0.406\n",
      "Epoch     6 avg_err =    0.409\n",
      "Epoch     7 avg_err =    0.412\n",
      "Epoch     8 avg_err =    0.410\n",
      "Epoch     9 avg_err =    0.402\n",
      "Epoch    10 avg_err =    0.394\n",
      "Epoch    11 avg_err =    0.393\n",
      "Epoch    12 avg_err =    0.362\n",
      "Epoch    13 avg_err =    0.370\n",
      "Epoch    14 avg_err =    0.345\n",
      "Epoch    15 avg_err =    0.355\n",
      "Epoch    16 avg_err =    0.340\n",
      "Epoch    17 avg_err =    0.349\n",
      "Epoch    18 avg_err =    0.357\n",
      "Epoch    19 avg_err =    0.332\n",
      "Epoch    20 avg_err =    0.326\n",
      "Epoch    21 avg_err =    0.347\n",
      "Epoch    22 avg_err =    0.303\n",
      "Epoch    23 avg_err =    0.342\n",
      "Epoch    24 avg_err =    0.324\n",
      "Epoch    25 avg_err =    0.340\n",
      "Epoch    26 avg_err =    0.334\n",
      "Epoch    27 avg_err =    0.308\n",
      "Epoch    28 avg_err =    0.326\n",
      "Epoch    29 avg_err =    0.332\n",
      "Epoch    30 avg_err =    0.327\n",
      "Epoch    31 avg_err =    0.316\n",
      "Epoch    32 avg_err =    0.311\n",
      "Epoch    33 avg_err =    0.327\n",
      "Epoch    34 avg_err =    0.314\n",
      "Epoch    35 avg_err =    0.314\n",
      "Epoch    36 avg_err =    0.306\n",
      "Epoch    37 avg_err =    0.314\n",
      "Epoch    38 avg_err =    0.331\n",
      "Epoch    39 avg_err =    0.324\n",
      "Epoch    40 avg_err =    0.295\n",
      "Epoch    41 avg_err =    0.318\n",
      "Epoch    42 avg_err =    0.321\n",
      "Epoch    43 avg_err =    0.337\n",
      "Epoch    44 avg_err =    0.327\n",
      "Epoch    45 avg_err =    0.308\n",
      "Epoch    46 avg_err =    0.324\n",
      "Epoch    47 avg_err =    0.329\n",
      "Epoch    48 avg_err =    0.329\n",
      "Epoch    49 avg_err =    0.332\n",
      "Epoch    50 avg_err =    0.306\n",
      "Epoch    51 avg_err =    0.329\n",
      "Epoch    52 avg_err =    0.301\n",
      "Epoch    53 avg_err =    0.309\n",
      "Epoch    54 avg_err =    0.322\n",
      "Epoch    55 avg_err =    0.313\n",
      "Epoch    56 avg_err =    0.311\n",
      "Epoch    57 avg_err =    0.329\n",
      "Epoch    58 avg_err =    0.316\n",
      "Epoch    59 avg_err =    0.334\n",
      "Epoch    60 avg_err =    0.340\n",
      "Epoch    61 avg_err =    0.322\n",
      "Epoch    62 avg_err =    0.324\n",
      "Epoch    63 avg_err =    0.309\n",
      "Epoch    64 avg_err =    0.306\n",
      "Epoch    65 avg_err =    0.319\n",
      "Epoch    66 avg_err =    0.327\n",
      "Epoch    67 avg_err =    0.319\n",
      "Epoch    68 avg_err =    0.311\n",
      "Epoch    69 avg_err =    0.316\n",
      "Epoch    70 avg_err =    0.316\n",
      "Epoch    71 avg_err =    0.306\n",
      "Epoch    72 avg_err =    0.308\n",
      "Epoch    73 avg_err =    0.316\n",
      "Epoch    74 avg_err =    0.316\n",
      "Epoch    75 avg_err =    0.303\n",
      "Epoch    76 avg_err =    0.313\n",
      "Epoch    77 avg_err =    0.295\n",
      "Epoch    78 avg_err =    0.308\n",
      "Epoch    79 avg_err =    0.319\n",
      "Epoch    80 avg_err =    0.305\n",
      "Epoch    81 avg_err =    0.322\n",
      "Epoch    82 avg_err =    0.308\n",
      "Epoch    83 avg_err =    0.321\n",
      "Epoch    84 avg_err =    0.326\n",
      "Epoch    85 avg_err =    0.319\n",
      "Epoch    86 avg_err =    0.305\n",
      "Epoch    87 avg_err =    0.301\n",
      "Epoch    88 avg_err =    0.331\n",
      "Epoch    89 avg_err =    0.324\n",
      "Epoch    90 avg_err =    0.301\n",
      "Epoch    91 avg_err =    0.316\n",
      "Epoch    92 avg_err =    0.339\n",
      "Epoch    93 avg_err =    0.316\n",
      "Epoch    94 avg_err =    0.309\n",
      "Epoch    95 avg_err =    0.298\n",
      "Epoch    96 avg_err =    0.326\n",
      "Epoch    97 avg_err =    0.316\n",
      "Epoch    98 avg_err =    0.318\n",
      "Epoch    99 avg_err =    0.309\n",
      "Epoch   100 avg_err =    0.314\n"
     ]
    }
   ],
   "source": [
    "#1.\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Dataset metadata\n",
    "dataset_url = \"https://drive.google.com/uc?authuser=0&id=1uxN2XunIHEhtv03dHgM9gE_J8i4QvKzU&export=download\"\n",
    "filename = 'diabetes.csv'\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "# Load Dataset\n",
    "def load_dataset():\n",
    "    ## Download and write diabetes data\n",
    "    if not (os.path.exists(filename)):\n",
    "        r = requests.get(dataset_url, allow_redirects=True)\n",
    "        open(filename, 'wb').write(r.content)\n",
    "    \n",
    "    ## Read diabetes data\n",
    "    dataset = np.genfromtxt(filename, delimiter=',', skip_header=1)\n",
    "    return dataset\n",
    "\n",
    "# Perceptron\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    Model Perceptron\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape, lr=1e-2, epochs=100, verbose=False):\n",
    "        \"\"\"\n",
    "        Weights initializer : Random\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(input_shape + 1) * 1e-2\n",
    "        self.bias = 1\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def _predict(self, x):\n",
    "        x = np.insert(x, 0, self.bias)\n",
    "        sigma = self.weights.T.dot(x)\n",
    "        output = self.activation_fn(sigma)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(x) for x in X])\n",
    "        \n",
    "    def activation_fn(self, x):\n",
    "        \"\"\"\n",
    "        Step function\n",
    "        \"\"\"\n",
    "        return int(x >= 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Online Learning Perceptron with Gradient Descent Training\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            print(\"Fitting/Learning process is running ...\")\n",
    "        \n",
    "        epch_msg = \"Epoch {:5} avg_err = {:8.3f}\"\n",
    "        for i in range(self.epochs):\n",
    "            err_summary = []\n",
    "            \n",
    "            for x, t in zip(X, y):\n",
    "                output = self._predict(x)\n",
    "                err = t - output\n",
    "                x = np.insert(x, 0, self.bias)\n",
    "                err_summary.append(abs(err))\n",
    "                \n",
    "                self.weights = self.weights + self.lr * err * x\n",
    "                self.bias = self.bias + self.lr * err\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(epch_msg.format(i + 1, sum(err_summary)/len(err_summary)))\n",
    "        \n",
    "# Split dataset\n",
    "dataset = load_dataset()\n",
    "split_point = int(dataset.shape[0]*0.8)\n",
    "X, y = dataset[:, :-1], dataset[:, -1]\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "\n",
    "# Jawaban soal\n",
    "ans = \"\"\"\\033[1m\n",
    "Fungsi aktivasi yang digunakan pada model Perceptron ini adalah step function sederhana.\n",
    "Fungsi aktivasi menentukan hasil klasifikasi berdasarkan `sign` atau `tanda` dari hasil jumlahan\n",
    "perkalian weights dengan data saja.\n",
    "\n",
    "Untuk pemilihan parameter learning rate, digunakan linear search atau mencoba satu per satu\n",
    "nilai dari learning rate yang memungkinkan. Untuk setiap hasil model dengan learning rate\n",
    "masing-masing akan dibandingkan nilai akurasinya.\n",
    "\\033[0m\n",
    "\"\"\"\n",
    "print(ans)\n",
    "\n",
    "# Model Training (Test, parameter tuning di cell nomor 2)\n",
    "model = Perceptron(X.shape[1], lr=1e-1, epochs=100, verbose=True)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "text",
    "id": "cWr_gBOgXU0Y",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "Learning rates : 1e-05\n",
      "=======================================\n",
      "          TP  precision  recall\n",
      "class 0   64  0.653061   0.646465\n",
      "class 1   21  0.375000   0.381818\n",
      "Overall accuracy : 0.551948\n",
      "=======================================\n",
      "Learning rates : 0.0001\n",
      "=======================================\n",
      "          TP  precision  recall\n",
      "class 0   71  0.669811   0.717172\n",
      "class 1   20  0.416667   0.363636\n",
      "Overall accuracy : 0.590909\n",
      "=======================================\n",
      "Learning rates : 0.001\n",
      "=======================================\n",
      "          TP  precision  recall\n",
      "class 0   88  0.661654   0.888889\n",
      "class 1   10  0.476190   0.181818\n",
      "Overall accuracy : 0.636364\n",
      "=======================================\n",
      "Learning rates : 0.01\n",
      "=======================================\n",
      "          TP  precision  recall\n",
      "class 0   93  0.659574   0.939394\n",
      "class 1    7  0.538462   0.127273\n",
      "Overall accuracy : 0.649351\n",
      "=======================================\n",
      "Learning rates : 0.1\n",
      "=======================================\n",
      "          TP  precision  recall\n",
      "class 0   86  0.803738   0.868687\n",
      "class 1   34  0.723404   0.618182\n",
      "Overall accuracy : 0.779221\n",
      "=======================================\n",
      "Learning rates : 1.0\n",
      "=======================================\n",
      "          TP  precision  recall\n",
      "class 0   83  0.734513   0.838384\n",
      "class 1   25  0.609756   0.454545\n",
      "Overall accuracy : 0.701299\n",
      "=======================================\n",
      "Learning rates : 10.0\n",
      "=======================================\n",
      "          TP  precision  recall\n",
      "class 0   93  0.744000   0.939394\n",
      "class 1   23  0.793103   0.418182\n",
      "Overall accuracy : 0.753247\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "#2.\n",
    "\n",
    "# Evaluation metrics\n",
    "\n",
    "def true_positive(y_pred, y_test, c):\n",
    "    c_test_idx = y_test == c\n",
    "    tp = np.sum([y_pred[c_test_idx] == c])\n",
    "    return int(tp)\n",
    "\n",
    "def precision(y_pred, y_test, c):\n",
    "    c_pred_idx = y_pred == c\n",
    "#     tp = np.sum([y_test[c_pred_idx] == c])\n",
    "#     fp = np.sum([y_test[c_pred_idx] != c])\n",
    "    return true_positive(y_pred, y_test, c)/np.sum(c_pred_idx)\n",
    "\n",
    "def recall(y_pred, y_test, c):\n",
    "    c_test_idx = y_test == c\n",
    "#     tp = np.sum([y_pred[c_test_idx] == c])\n",
    "#     fn = np.sum([y_pred[c_test_idx] != c])\n",
    "    return true_positive(y_pred, y_test, c)/np.sum(c_test_idx)\n",
    "\n",
    "def evaluate(y_pred, y_test):\n",
    "    classes = np.unique(y_test)\n",
    "    print(\"%7s %4s %10s %7s\" % (\"\", \"TP\", \"precision\", \"recall\"))\n",
    "    summary = []\n",
    "    \n",
    "    for c in classes:\n",
    "        tp = true_positive(y_pred, y_test, c)\n",
    "        pre = precision(y_pred, y_test, c)\n",
    "        rec = recall(y_pred, y_test, c)\n",
    "        summary.append((tp, pre, rec))\n",
    "        print(\"%s %4d  %2f   %2f\" % (\"class \" + str(int(c)), tp, pre, rec))\n",
    "    \n",
    "    print(\"Overall accuracy : %2f\" % (sum([e[0] for e in summary])/y_test.shape[0]))\n",
    "    return summary\n",
    "\n",
    "# Linear Search Parameter Tuning\n",
    "\n",
    "SEED = 42\n",
    "learning_rates = np.logspace(-5,1,7)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(\"=======================================\")\n",
    "    print(\"Learning rates :\", lr)\n",
    "    print(\"=======================================\")\n",
    "    model = Perceptron(X.shape[1], lr=lr, epochs=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    summary = evaluate(y_pred, y_test)\n",
    "\n",
    "print(\"=======================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nM8fuH40XU0r"
   },
   "source": [
    "<b><i>Ensemble Learning</i></b>\n",
    "\n",
    "3.Dengan proporsi training dan testing yang sama dengan soal nomor 1, lakukan prediksi pada dataset diabetes dengan ketentuan sebagai berikut.\n",
    "\n",
    "Buatlah <i> bagging classifier</i> dengan menggunakan Perceptron yang Anda buat sebagai<i> base learner </i>. Lakukan variasi terhadap jumlah <i>bootstrap sample</i>. Dari prediksi bagging perceptron, tampilkan akurasi, precision dan recall per kelas pada tiap variasi jumlah <i>bootstrap sample</i> (bisa berupa tabel atau grafik).\n",
    "<br><br>\n",
    "4.Dengan proporsi training dan testing yang sama dengan soal nomor 1, lakukan prediksi pada dataset diabetes dengan ketentuan sebagai berikut.\n",
    "\n",
    "Implementasikan <i>bagging classifier</i> yang sudah Anda buat dengan perubahan pada <i> base learner </i>. Pada eksperimen ini, gunakan multilayer perceptron (MLP) sebagai <i> base learner </i>. Anda dapat menggunakan <i>library</i> untuk mengimplementasikan MLP (Sklearn atau Keras). Anda dapat melakukan variasi terhadap jumlah bootstrap sample, jumlah hidden layer, jumlah output unit, jumlah hidden unit, atau variasi pada fungsi aktivasi. Tampilkan akurasi, precision dan recall per kelas pada tiap variasi yang Anda lakukan (bisa berupa tabel atau grafik). \n",
    "<br><br>\n",
    "5.Lakukan komparasi dan analisis dari keseluruhan hasil eksperimen yang Anda lakukan (Perceptron, BaggingPperceptron, Bagging MLP) untuk menyelesaikan permasalahan klasifikasi penyakit diabetes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "5PDtwcEuXU0t"
   },
   "source": [
    "Jawaban:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab_type": "text",
    "id": "utTigxsaXU0z",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "============================================\n",
      "          TP  precision  recall\n",
      "class 0   55  0.820896   0.555556\n",
      "class 1   43  0.494253   0.781818\n",
      "Overall accuracy : 0.636364\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "============================================\n",
      "          TP  precision  recall\n",
      "class 0   50  0.781250   0.505051\n",
      "class 1   41  0.455556   0.745455\n",
      "Overall accuracy : 0.590909\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "============================================\n",
      "          TP  precision  recall\n",
      "class 0   89  0.741667   0.898990\n",
      "class 1   24  0.705882   0.436364\n",
      "Overall accuracy : 0.733766\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "#3. \n",
    "\n",
    "class BaggingClassfier:\n",
    "    def __init__(self, model, n_samples=3):\n",
    "        self.n_samples = n_samples\n",
    "        self.models = [model]*n_samples\n",
    "    \n",
    "    def _sampling(self, X, y):\n",
    "        \"\"\"\n",
    "        Bootstrap Sampling\n",
    "        \"\"\"\n",
    "        n_rows = X.shape[0]\n",
    "        sample_idx = np.random.randint(n_rows, size=n_rows)\n",
    "        return X[sample_idx], y[sample_idx]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Bootstrap Aggregation Learning\n",
    "        \"\"\"\n",
    "        for i in range(self.n_samples):\n",
    "            X_sample, y_sample = self._sampling(X, y)\n",
    "            self.models[i].fit(X_sample, y_sample)\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # predict per models\n",
    "        y_preds = []\n",
    "        \n",
    "        for i in range(self.n_samples):\n",
    "            y_pred = self.models[i].predict(X)\n",
    "            y_preds.append(y_pred)\n",
    "        \n",
    "        # major vote\n",
    "        y_preds = np.array(y_preds).T\n",
    "        y_pred = np.array([self._major_vote(y_preds[i]) for i in range(y_preds.shape[0])])\n",
    "        return y_pred\n",
    "    \n",
    "    def _major_vote(self, predictions):\n",
    "        \"\"\"\n",
    "        Major voting\n",
    "        \"\"\"\n",
    "        values, counts = np.unique(predictions, return_counts=True)\n",
    "        idx = np.argmax(counts)\n",
    "        return values[idx]\n",
    "\n",
    "# Split dataset\n",
    "dataset = load_dataset()\n",
    "X, y = dataset[:, :-1], dataset[:, -1]\n",
    "split_point = int(X.shape[0]*0.8)\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "\n",
    "# Linear Search Parameter Tuning\n",
    "\n",
    "SEED = 42\n",
    "n_samples_param = np.linspace(3, 7, 3)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "for n_samples in n_samples_param:\n",
    "    print(\"============================================\")\n",
    "    print(\"Number of Learners (Bootstrap Samples) :\", int(n_samples))\n",
    "    print(\"============================================\")\n",
    "    model = Perceptron(X.shape[1], lr=1e-1, epochs=100)\n",
    "    model1 = BaggingClassfier(model=model, n_samples=int(n_samples))\n",
    "    model1.fit(X_train, y_train)\n",
    "    y_pred = model1.predict(X_test)\n",
    "    summary = evaluate(y_pred, y_test)\n",
    "\n",
    "print(\"============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab_type": "text",
    "id": "RcBdp4GQXU02",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   99  0.642857   1.000000\n",
      "class 1    0  nan   0.000000\n",
      "Overall accuracy : 0.642857\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   87  0.707317   0.878788\n",
      "class 1   19  0.612903   0.345455\n",
      "Overall accuracy : 0.688312\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   70  0.707071   0.707071\n",
      "class 1   26  0.472727   0.472727\n",
      "Overall accuracy : 0.623377\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   99  0.642857   1.000000\n",
      "class 1    0  nan   0.000000\n",
      "Overall accuracy : 0.642857\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   81  0.648000   0.818182\n",
      "class 1   11  0.379310   0.200000\n",
      "Overall accuracy : 0.597403\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   87  0.690476   0.878788\n",
      "class 1   16  0.571429   0.290909\n",
      "Overall accuracy : 0.668831\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   99  0.642857   1.000000\n",
      "class 1    0  nan   0.000000\n",
      "Overall accuracy : 0.642857\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   80  0.661157   0.808081\n",
      "class 1   14  0.424242   0.254545\n",
      "Overall accuracy : 0.610390\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 3\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   91  0.684211   0.919192\n",
      "class 1   13  0.619048   0.236364\n",
      "Overall accuracy : 0.675325\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   83  0.628788   0.838384\n",
      "class 1    6  0.272727   0.109091\n",
      "Overall accuracy : 0.577922\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   71  0.689320   0.717172\n",
      "class 1   23  0.450980   0.418182\n",
      "Overall accuracy : 0.610390\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   92  0.652482   0.929293\n",
      "class 1    6  0.461538   0.109091\n",
      "Overall accuracy : 0.636364\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   99  0.642857   1.000000\n",
      "class 1    0  nan   0.000000\n",
      "Overall accuracy : 0.642857\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   72  0.692308   0.727273\n",
      "class 1   23  0.460000   0.418182\n",
      "Overall accuracy : 0.616883\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   79  0.718182   0.797980\n",
      "class 1   24  0.545455   0.436364\n",
      "Overall accuracy : 0.668831\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   99  0.642857   1.000000\n",
      "class 1    0  nan   0.000000\n",
      "Overall accuracy : 0.642857\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   64  0.688172   0.646465\n",
      "class 1   26  0.426230   0.472727\n",
      "Overall accuracy : 0.584416\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 5\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   62  0.720930   0.626263\n",
      "class 1   31  0.455882   0.563636\n",
      "Overall accuracy : 0.603896\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   99  0.642857   1.000000\n",
      "class 1    0  nan   0.000000\n",
      "Overall accuracy : 0.642857\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   85  0.726496   0.858586\n",
      "class 1   23  0.621622   0.418182\n",
      "Overall accuracy : 0.701299\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (50, 100)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   85  0.714286   0.858586\n",
      "class 1   21  0.600000   0.381818\n",
      "Overall accuracy : 0.688312\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   81  0.632812   0.818182\n",
      "class 1    8  0.307692   0.145455\n",
      "Overall accuracy : 0.577922\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   88  0.661654   0.888889\n",
      "class 1   10  0.476190   0.181818\n",
      "Overall accuracy : 0.636364\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (100, 100)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   95  0.673759   0.959596\n",
      "class 1    9  0.692308   0.163636\n",
      "Overall accuracy : 0.675325\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : logistic\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   99  0.642857   1.000000\n",
      "class 1    0  nan   0.000000\n",
      "Overall accuracy : 0.642857\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : tanh\n",
      "============================================\n",
      "Result :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          TP  precision  recall\n",
      "class 0   90  0.692308   0.909091\n",
      "class 1   15  0.625000   0.272727\n",
      "Overall accuracy : 0.681818\n",
      "============================================\n",
      "Number of Learners (Bootstrap Samples) : 7\n",
      "Hidden Layer : (50, 100, 50)\n",
      "Activation Function : relu\n",
      "============================================\n",
      "Result :\n",
      "          TP  precision  recall\n",
      "class 0   80  0.720721   0.808081\n",
      "class 1   24  0.558140   0.436364\n",
      "Overall accuracy : 0.675325\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Linear Search Parameter Tuning\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "n_samples_param = np.linspace(3, 7, 3)\n",
    "hidden_layer_param = [(50, 100), (100, 100), (50, 100, 50)]\n",
    "activation_param = [\"logistic\", \"tanh\", \"relu\"]\n",
    "\n",
    "# jumlah bootstrap sample, jumlah hidden layer, jumlah output unit, jumlah hidden unit, atau variasi pada fungsi aktivasi\n",
    "for n_samples in n_samples_param:\n",
    "    for hidden_layer in hidden_layer_param:\n",
    "        for activation in activation_param:\n",
    "            print(\"============================================\")\n",
    "            print(\"Number of Learners (Bootstrap Samples) :\", int(n_samples))\n",
    "            print(\"Hidden Layer :\", hidden_layer)\n",
    "            print(\"Activation Function :\", activation)\n",
    "            print(\"============================================\")\n",
    "            print(\"Result :\")\n",
    "            model = MLPClassifier(hidden_layer_sizes=hidden_layer, max_iter=500, activation=activation,\n",
    "                                  alpha=1e-4, solver='sgd', random_state=42, tol=1e-4)\n",
    "            model1 = BaggingClassfier(model=model, n_samples=int(n_samples))\n",
    "            model1.fit(X_train, y_train)\n",
    "            y_pred = model1.predict(X_test)\n",
    "            summary = evaluate(y_pred, y_test)\n",
    "\n",
    "print(\"============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab_type": "text",
    "id": "mS0k4yE_XU06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "Berdasarkan hasil eksperimen yang saya lakukan, berikut kesimpulan untuk setiap jenis klasifikasi :\n",
      "\n",
      "1. Untuk single Perceptron, hasil terbaik didapatkan saat learning rates 0.1 dengan detail sebagai berikut.\n",
      "\n",
      "    =======================================\n",
      "    Learning rates : 0.1\n",
      "    =======================================\n",
      "              TP  precision  recall\n",
      "    class 0   86  0.803738   0.868687\n",
      "    class 1   34  0.723404   0.618182\n",
      "    Overall accuracy : 0.779221\n",
      "    =======================================\n",
      "    \n",
      "   Dapat dilihat bahwa sudah berhasil didapatkan akurasi sebesar 0.77.\n",
      "\n",
      "2. Untuk Bagging Perceptron, hasil terbaik didapatkan saat banyak learner adalah 7 dengan detail sebagai berikut.\n",
      "\n",
      "    ============================================\n",
      "    Number of Learners (Bootstrap Samples) : 7\n",
      "    ============================================\n",
      "              TP  precision  recall\n",
      "    class 0   89  0.741667   0.898990\n",
      "    class 1   24  0.705882   0.436364\n",
      "    Overall accuracy : 0.733766\n",
      "    ============================================\n",
      "\n",
      "   Dapat dilihat bahwa akurasi yang didapatkan hanya 73%, lebih rendah dari single Perceptron\n",
      "   \n",
      "3. Untuk Bagging MLP, hasil terbaik didapatkan saat parameter yang digunakan seperti berikut ini.\n",
      "\n",
      "    ============================================\n",
      "    Number of Learners (Bootstrap Samples) : 7\n",
      "    Hidden Layer : (50, 100)\n",
      "    Activation Function : tanh\n",
      "    ============================================\n",
      "    Result :\n",
      "              TP  precision  recall\n",
      "    class 0   85  0.726496   0.858586\n",
      "    class 1   23  0.621622   0.418182\n",
      "    Overall accuracy : 0.701299\n",
      "    ============================================\n",
      "   \n",
      "   Dapat dilihat bahwa akurasi yang didapatkan sekitar 70% dan paling rendah di antara yang lain.\n",
      "   \n",
      "\n",
      "Sehingga saya menyimpulkan untuk permasalahan klasifikasi penyakit diabetes ini, Perceptron sudah cukup \n",
      "baik untuk melakukan klasifikasi. Untuk penggunaan bagging, sebenarnya cukup berhasil membuat trainig lebih\n",
      "baik (dari konsistensi beberapa kali percobaan). Hal yang dapat dikembangkan adalah cara melakukan voting-nya. \n",
      "Hal yang membuat MLP kurang baik akurasinya adalah kurangnya tuning hyperparameter.\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "\n",
    "# Lakukan komparasi dan analisis dari keseluruhan hasil eksperimen yang Anda lakukan \n",
    "# (Perceptron, BaggingPperceptron, Bagging MLP) untuk menyelesaikan permasalahan klasifikasi penyakit diabetes.\n",
    "\n",
    "ans = \"\"\"\\033[1m\n",
    "Berdasarkan hasil eksperimen yang saya lakukan, berikut kesimpulan untuk setiap jenis klasifikasi :\n",
    "\n",
    "1. Untuk single Perceptron, hasil terbaik didapatkan saat learning rates 0.1 dengan detail sebagai berikut.\n",
    "\n",
    "    =======================================\n",
    "    Learning rates : 0.1\n",
    "    =======================================\n",
    "              TP  precision  recall\n",
    "    class 0   86  0.803738   0.868687\n",
    "    class 1   34  0.723404   0.618182\n",
    "    Overall accuracy : 0.779221\n",
    "    =======================================\n",
    "    \n",
    "   Dapat dilihat bahwa sudah berhasil didapatkan akurasi sebesar 0.77.\n",
    "\n",
    "2. Untuk Bagging Perceptron, hasil terbaik didapatkan saat banyak learner adalah 7 dengan detail sebagai berikut.\n",
    "\n",
    "    ============================================\n",
    "    Number of Learners (Bootstrap Samples) : 7\n",
    "    ============================================\n",
    "              TP  precision  recall\n",
    "    class 0   89  0.741667   0.898990\n",
    "    class 1   24  0.705882   0.436364\n",
    "    Overall accuracy : 0.733766\n",
    "    ============================================\n",
    "\n",
    "   Dapat dilihat bahwa akurasi yang didapatkan hanya 73%, lebih rendah dari single Perceptron\n",
    "   \n",
    "3. Untuk Bagging MLP, hasil terbaik didapatkan saat parameter yang digunakan seperti berikut ini.\n",
    "\n",
    "    ============================================\n",
    "    Number of Learners (Bootstrap Samples) : 7\n",
    "    Hidden Layer : (50, 100)\n",
    "    Activation Function : tanh\n",
    "    ============================================\n",
    "    Result :\n",
    "              TP  precision  recall\n",
    "    class 0   85  0.726496   0.858586\n",
    "    class 1   23  0.621622   0.418182\n",
    "    Overall accuracy : 0.701299\n",
    "    ============================================\n",
    "   \n",
    "   Dapat dilihat bahwa akurasi yang didapatkan sekitar 70% dan paling rendah di antara yang lain.\n",
    "   \n",
    "\n",
    "Sehingga saya menyimpulkan untuk permasalahan klasifikasi penyakit diabetes ini, Perceptron sudah cukup \n",
    "baik untuk melakukan klasifikasi. Untuk penggunaan bagging, sebenarnya cukup berhasil membuat trainig lebih\n",
    "baik (dari konsistensi beberapa kali percobaan). Hal yang dapat dikembangkan adalah cara melakukan voting-nya. \n",
    "Hal yang membuat MLP kurang baik akurasinya adalah kurangnya tuning hyperparameter.\n",
    "\n",
    "\\033[0m\"\"\"\n",
    "\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tugas+1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
